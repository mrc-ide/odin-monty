---
title: "Fitting odin models with monty"
format:
  revealjs:
    preview-links: auto
    footer: "[mrc-ide/odin-monty](..)"
execute:
  echo: true
  message: true
  output: true
  warning: true
---

```{r}
#| include: false
#| cache: false
source("../common.R")
file.copy("../data/incidence.csv", ".", overwrite = TRUE)
```
# A pragmatic introduction

```{r}
library(odin2)
library(dust2)
library(monty)
```

## Previously on "Introduction to odin"

* We created some simple compartmental models
* We ran these and observed trajectories over time
* We saw that stochastic models produce a family of trajectories

## Our model {.smaller}

```{r}
#| echo: false
#| results: "asis"
r_output(readLines("models/sir-basic.R"))
```

## Example output {.smaller}

```{r}
#| include: false
sir <- odin("models/sir-basic.R")
```

```{r}
sys <- dust_system_create(sir, n_particles = 30)
dust_system_set_state_initial(sys)
t <- seq(0, 100)
y <- dust_system_simulate(sys, t)
incidence <- dust_unpack_state(sys, y)$incidence
matplot(t, t(incidence), type = "l", lty = 1, col = "#00000033",
        xlab = "Time", ylab = "Incidence")
```

## How do we fit this to data?

We need:

* a data set
  - time series of observed data (incidence? prevalance? something else?)
* a measure of goodness of fit
  - how do we cope with stochasticity?
* to know what parameters we are trying to fit

## The data {.smaller}

You should download [incidence.csv](../incidence.csv)

```{r}
data <- read.csv("incidence.csv")
head(data)
```

We will fit **`cases`** here to **`incidence`** in our model.

## The data {.smaller}

```{r}
plot(data, pch = 19, col = "red")
```

## Measuring goodness of fit {.smaller}

Run the system with `beta = 0.4` and `gamma = 0.2`

```{r}
sys <- dust_system_create(sir, list(beta = 0.4, gamma = 0.2))
dust_system_set_state_initial(sys)
idx <- dust_unpack_index(sys)$incidence
t <- data$time
y <- dust_system_simulate(sys, t, index_state = idx)
```

## Measuring goodness of fit {.smaller}

```{r}
plot(data, col = "red", pch = 19, ylim = c(0, max(data$cases)))
points(t, drop(y), col = "blue", pch = 19)
segments(data$time, data$cases, y1 = drop(y))
```

## Measuring goodness of fit {.smaller}

$$
P(\mathrm{data} | \mathrm{model})
$$

perhaps:

$$
P(\mathrm{observed\ cases}) \sim \mathrm{Poisson}(\mathrm{modelled\ cases})
$$

```{r}
dpois(data$cases, drop(y), log = TRUE)
```

## Adding goodness-of-fit to the model {.smaller}

```{r}
#| echo: false
#| results: "asis"
r_output(readLines("models/sir-compare.R"))
```

```{r}
#| include: false
sir <- odin("models/sir-compare.R")
```

## Measuring goodness-of-fit {.smaller}

```{r}
filter <- dust_filter_create(sir, data = data, time_start = 0,
                             n_particles = 200)
dust_likelihood_run(filter, list(beta = 0.4, gamma = 0.2))
```

. . .

The system runs stochastically, and the likelihood is different each time:

```{r}
dust_likelihood_run(filter, list(beta = 0.4, gamma = 0.2))
dust_likelihood_run(filter, list(beta = 0.4, gamma = 0.2))
```

## Filtered trajectories {.smaller}

```{r}
dust_likelihood_run(filter, list(beta = 0.4, gamma = 0.2),
                    save_trajectories = TRUE, index_state = idx)
y <- dust_likelihood_last_trajectories(filter)
matplot(data$time, t(drop(y)), type = "l", col = "#00000033", lty = 1,
        xlab = "Time", ylab = "Incidence")
points(data, pch = 19, col = "red")
```

# State space models


(aka, what is really going on here?)

## What is it?

- A state space model (SSM) is a mathematical framework for modelling a dynamical system.
- It is built around two processes:
    - **state equations** that describes the evolution of some latent variables (also referred as "hidden" states) over time
    - **observation equations** that relates the observations to the latent variables.

## Can you be more precise?

![](images/SSM.jpg)

- $x_{t, 1 \leq t \leq T}$ the hidden states of the system
- $y_{t, 1 \leq t \leq T}$ the observations
- $f_{\theta}$ the state transition function
- $g_{\theta}$ the observation function
- $t$ is often time
- $\theta$ defines the model

## Two common problems

![](images/SSM.jpg)

- Two common needs
  - "Filtering" i.e. estimate the hidden states $x_{t}$ from the observations $y_t$
  - "Inference" i.e. estimate the $\theta$'s compatible with the observations $y_{t}$

# (Bootstrap) Sequential Monte Carlo {.smaller}

AKA, the particle filter

- Assuming a given $\theta$, at each time step $t$, BSSMC:
  1. generates $X_{t+1}^N$ by using $f_{\theta}(X_{t+1}^N|X_{t}^N)$ (the $N$ particles)
  2. calculates weights for the newly generated states based on $g_{\theta}(Y_{t+1}|X_{t+1})$
  3. resamples the states to keep only the good ones

- Allow to explores efficiently the state space by progressively integrating the data points

- Produces a MC approximation of $p(Y_{1:T}|\theta)$ the marginal likelihood

## The filter in action

![](images/filter.gif)

# Particle MCMC

## What is Particle MCMC? {.smaller}

- PMCMC is an algorithm which performs "filtering" and "inference"
- A Markov Chain Monte Carlo (MCMC) method for estimating target distributions
- MCMC explores the parameter space by moving randomly making jumps from one value to the next
- Probability of going from point to the other is determined by the proposal distribution and the ratio of the likelihood
- Compared with "traditional" MCMC, in PMCMC, the likelihood estimation is approximated using a "particle filter"
- The filter generates a set of "particles" i.e. trajectories compatible with the observation
- It uses these trajectories to compute a (marginal) likelihood that can be use by the PMCMC

## Core algorithm

1. **Initialisation** Start with a value $\theta_{0}$ from the parameter space
2. **Initial SMC** Use sequential Monte Carlo to do the "filtering" and samples of potential $\{X_{t}\}_{1..N}$. Calculate the (marginal) likelihood from this using a MC estimator
3. **Proposal** Propose a new parameter value $\theta ^*$
4. **SMC** Calculate marginal likelihood of proposal
5. **Metropolis-Hastings** Accept with probability $\min(1, \alpha)$ with $\alpha = \frac{p(\theta ^*)}{p(\theta_{t})} \cdot \frac{q(\theta_{t})}{q(\theta ^*)}$
6. **Loop** Redo (3) until the number of steps is reached

# monty & dust2 {.smaller}

- `dust2`
  - implements bootstrap particle filter
  - can run models in parallel

- `monty`
  - implements MCMC with support for stochastic probability densities
  - we are adding additional samplers (adaptive MCMC, HMC, but most can't be used with stochastic densities)

- `mcstate`
  - used to contain all of this (plus SMC^2, IF)
  - Inference tooling for the Centre's UK COVID model & other diseases

## Design philosophy

- More complex structures are built up from simpler objects
  - Filter {data, model, n_particles}
  - PMCMC {parameters, filter}
- Provides you with low-level tools, and little handholding
- Pretty fast though

# PMCMC {.smaller}

We have a marginal likelihood estimator from our particle filter:

```{r}
dust_likelihood_run(filter, list(beta = 0.4, gamma = 0.2))
```

. . .

How do we sample from `beta` and `gamma`?

. . .

We need:

* to tidy up our parameters
* to create a prior
* to create a posterior
* to create a sampler

## "Parameters" {.smaller}

* Our filter takes a **list** of `beta` and `gamma`, `pars`
  - it could take all sorts of other things, not all of which are to be estimated
  - some of the inputs might be vectors or matrices
* Our MCMC takes an **unstructured vector** $\theta$
  - we propose a new $\theta^*$ via some kernel, say a multivariate normal requiring a matrix of parameters corresponding to $\theta$
  - we need a prior over $\theta$, but not necessarily every element of `pars`
* Smoothing this over is a massive nuisance
  - some way of mapping from $\theta$ to `pars` (and back again)

## Parameter packers {.smaller}

Our solution, "packers"

```{r}
packer <- monty_packer(c("beta", "gamma"))
packer
```

. . .

We can transform from $\theta` to a named list:

```{r}
packer$unpack(c(0.2, 0.1))
```

. . .

and back the other way:

```{r}
packer$pack(c(beta = 0.2, gamma = 0.1))
```

## Parameter packers {.smaller}

Bind additional data

```{r}
packer <- monty_packer(c("beta", "gamma"), fixed = list(I0 = 5))
packer$unpack(c(0.2, 0.1))
```

## Parameter packers {.smaller}

Cope with vector-valued parameters in $\theta$

```{r}
packer <- monty_packer(array = c(beta = 3, gamma = 3))
packer
packer$unpack(c(0.2, 0.21, 0.22, 0.1, 0.11, 0.12))
```

## Priors {.smaller}

Another DSL, similar to odin's:

```{r}
prior <- monty_dsl({
  beta ~ Exponential(mean = 0.5)
  gamma ~ Exponential(mean = 0.3)
})
prior
```

This is a "monty model"

```{r}
monty_model_density(prior, c(0.2, 0.1))
```

compute this density manually:

```{r}
dexp(0.2, 1 / 0.5, log = TRUE) + dexp(0.1, 1 / 0.3, log = TRUE)
```

## From a dust filters to a monty model {.smaller}

```{r}
filter
```

. . .

Combine a filter and a packer

```{r}
packer <- monty_packer(c("beta", "gamma"))
likelihood <- dust_likelihood_monty(filter, packer)
likelihood
```

## Posterior from likelihood and prior {.smaller}

Combine a likelihood and a prior (any two models)

```{r}
posterior <- likelihood + prior
posterior
```

## Create a sampler

A diagonal variance-covariance matrix (uncorrelated parameters)

```{r}
vcv <- diag(2) * 0.2
vcv
```

Use this as the argument to a sampler

```{r}
sampler <- monty_sampler_random_walk(vcv)
sampler
```

## Let's sample!

```{r, cache = TRUE}
samples <- monty_sample(posterior, sampler, 1000)
samples
```

## The result: parameters

```{r}
plot(t(drop(samples$pars)))
```

## The result: density over time

```{r}
plot(samples$density, type = "l")
```

## The result: density over time

```{r}
plot(samples$density[-(1:100), ], type = "l")
```

## Better mixing {.smaller}

```{r, cache = TRUE}
vcv <- matrix(c(0.005, 0.003, 0.003, 0.003), 2, 2)
sampler <- monty_sampler_random_walk(vcv)
samples <- monty_sample(posterior, sampler, 5000, n_chains = 4)
matplot(samples$density[-(1:200), ], type = "l", lty = 1)
```

# Parallelism

Two places to parallelise

* among particles in your filter
* between chains in the sample

e.g., 4 threads per filter x 2 workers = 8 total cores in use

## Configure the filter

Use the `n_threads` argument, here for 4 threads

```{r}
filter <- dust_filter_create(sir, data = data, time_start = 0,
                             n_particles = 200, n_threads = 4)
```

requires that you have OpenMP; this is very annoying on macOS

## Configure a parallel runner

Use `monty_runner_callr`, here for 2 workers

```{r}
runner <- monty_runner_callr(2)
```

Pass `runner` through to `monty_sample`:

```{r, eval = FALSE}
samples <- monty_sample(posterior, sampler, 1000,
                        runner = runner, n_chains = 4)
```
