# 2009 pandemic in the UK

```{r}
#| include: false
source("common.R")
```

```{r, echo=FALSE}
r_output <- function(path, highlight = NULL) {
  if (is.null(highlight)) {
    prefix <- "```r"
  } else {
    prefix <- sprintf('```{.r code-line-numbers="%s"}', highlight)
  }
  writeLines(c(prefix, readLines(path), "```"))
}
set.seed(1) # always the same
knitr::knit_hooks$set(small_margins = function(before, options, envir) {
  if (before) {
    par(mar = c(4, 4, .1, .1))
  }
})
```
## Introduction

### Objective of this chapter

The 2009 A/H1N1 influenza pandemic posed a significant public health challenge in England and Wales, requiring rapid and evidence-based decision-making to mitigate its impact. This chapter explores the use of `odin` and `monty` to model the dynamics of the 2009 A/H1N1 influenza pandemic in England and Wales incorporating changes in social contacts during holiday periods. We demonstrate how to construct, fit, and analyse a compartmental model to infer key epidemiological parameters.

The chapter is structured to guide the reader through the entire modelling pipeline, from data preparation and visualisation to model construction, parameter inference, and validation. Along the way, we highlight the integration of real-world data, discuss challenges in linking models to observed cases, and showcase the use of Bayesian methods for robust parameter estimation.

### Overview of the pipeline

This chapter illustrates how to:

1. Construct an SEIR model with time-varying transmission using `odin`.
2. Implement an observation model, derive a likelihood based on observations using `odin`.
3. Run Bayesian inference using Markov chain Monte Carlo (MCMC) with `monty`.
4. Analyse and interpret MCMC results.

### Key parameters of interest


   - The basic reproduction number $R_0$.  
   - The ascertainment probability, i.e., the proportion of symptomatic infections reported as cases.

```{r}
library(odin2)
library(dust2)
library(monty)
```

## Data preparation and visualisation

Data is the number of weekly cases of A/H1N1 pandemic cases from June 2009, and initially broken in seven age-groups. For simplicity, we aggregate the data into a single time series for the analysis.

### Loading data

The data file is taken from the supplement material in [@endo_introduction_2019] and can be found [here](https://github.com/akira-endo/Intro-PMCMC/blob/master/assets/andre_estimates_21_02.txt).

We will fit our model to all cases aggregated so we need to load our data, add the age-group, and then build a dataframe (a table where each rows is an observations of certains variables in the columns) with 'Days' and 'cases'. We use the `dplyr` package, a popular R package for data handling, with the '%>%' ('pipe') operator but other methods are possible.  

```{r}
library(dplyr)

# Read the data
original_data <- read.csv(file = "data/andre_estimates_21_02.txt", sep = "\t") %>% rowSums()

# Group all age groups
data <- data.frame(Cases = original_data) %>%
  mutate(Days = seq(7, by = 7, length.out = n())) %>%
  dplyr::select(Days, Cases)
```

### Plotting the data

You can plot the data and see the two waves of infections and the presence of holiday periods.
```{r}
#plot the data
plot(data$Day, data$Cases, pch = 19, col = "red",
     xlab="Days since start of epidemics",
     ylab="Official cases count")

# Add the holiday periods - summer holidays and half-term
# Holiday calendar assuming 30% contact reduction during summer and 15% during half term
hol_t <- c(0,38,85,140,147)
hol_v <- c(1,.70,1,.85,1)
#summer holiday block
rect(xleft = hol_t[2],ybottom = 0, xright = hol_t[3], ytop = max(data$Cases)*1.2, col = "#aaaabb33", border = NA)
#half-term holiday block
rect(xleft = hol_t[4],ybottom = 0, xright = hol_t[5], ytop = max(data$Cases)*1.2, col = "#aaaabb33", border = NA)
```
## Model design

### Transmission model

Similarly to the SIR model, we saw already, the SEIR model divides the population into four compartments:

- **S**usceptible: Individuals at risk of infection.  
- **E**xposed: Infected individuals who are not yet infectious.  
- **I**nfectious: Actively transmitting the disease.  
- **R**ecovered: Immune individuals.  

Additionally, the model incorporates time-varying transmission rates to account for changes in contact patterns during holidays.

The SEIR model is governed by the following system of ordinary differential equations:

$$
\begin{aligned}
    \frac{dS}{dt} &= - \beta(t) \frac{S I}{N} \\
    \frac{dE}{dt} &= \beta(t) \frac{S I}{N} - \gamma E \\
    \frac{dI}{dt} &= \gamma - \sigma I \\
    \frac{dR}{dt} &= \sigma I
\end{aligned}
$$

The parameters for the transmission model are defined as follows:
$$
\begin{aligned}
    R_0 &\text{ (basic reproduction number): } R_0 = 1.5, \\
    \gamma &\text{ (rate of transition from exposed to infectious): } \gamma = \frac{1}{L}, \\
    \sigma &\text{ (recovery rate): } \sigma = \frac{1}{D}, \\
    \beta &\text{ (transmission rate): } \beta = R_0 \cdot \sigma, \\
    N &\text{ (total population): } N = 55,000,000, \\
    \text{hol} &\text{ (time-varying factor): interpolated from } h_{\text{times}} \text{ and } h_{\text{values}}.
\end{aligned}
$$

The initial conditions are:
$$
\begin{aligned}
    S(0) &= (1 - 2 \alpha) N \\
    E(0) &= \alpha N \\
    I(0) &= \alpha N \\
    R(0) &= 0 \\
\end{aligned}
$$

### Observation model

The fundamental component of our transmission model is the 'I' compartment tracking continuously the number of infectious people in our population (an infection prevalence). However, what we observe through our surveillance system is a fraction of the cumulative incidence, more precisely an estimate of the number of new cases in a period of 7 days (a week).

In our `odin` model we can track the weekly cumulative incidence $Z(t)$ by:

1. adding a differential equation integrating the instantaneous incidence:
$$\frac{dZ}{dt} = \gamma E$$

2. reseting $Z(t) = 0$ at the beginning of each week.

Then we get for each $t$ that is a multiple of 7:

$$Z(t) = \int_{t-7}^{t} \gamma E$$
i.e. the cumulative incidence over a week.

Finally, the observation model is defined as:
$$
\text{Cases(t)} \sim \text{NegativeBinomial}(\mu = \rho Z(t), \text{size} = \eta),
$$
where \(\rho\) is the ascertainment probability, and $\eta$ is the dispersion parameter.

### Odin code of the model

Below is the `odin` code for our model - the structure matches closely the mathematical formulation. Note the `Cases <- data()` lines telling that `Cases` are observations. The model will compile and work even when it is not linked with any dataset. However providing data is essentiel to compute a likelihood with `dust` or derive a `monty` statistical model out of it.

This step takes time. The model needs to be transpiled (e.g. translated from one programming language to another) in order to create some C++ code and then that code is compiled and loaded in the R environnment. After this, a "fast" model can be called from the R environment and used to simulate scenarios or perform inference. 

```{r, echo = TRUE}
seir <- odin2::odin({
# initial conditions
initial(S) <- (1 - 2 * alpha) * N
initial(E) <- alpha * N
initial(I) <- alpha * N
initial(R) <- 0
initial(incidence, zero_every = 7) <- 0

# equations
deriv(S) <- - hol * beta * S * I / N
deriv(E) <- hol * beta * S * I / N - gamma * E
deriv(I) <- gamma * E - sigma * I
deriv(R) <- sigma * I
deriv(incidence) <- gamma * E

# parameter values
R_0 <- parameter(1.5)
L <- 1
D <- 1.25
alpha <- parameter(1e-4) # initial proportion
N <- 55000000

# convert parameters
hol <- interpolate(h_times, h_values, "constant")
h_times <- parameter()
h_values <- parameter()
dim(h_times) <- parameter(rank=1)
dim(h_values) <- length(h_times)
gamma <- 1 / L
sigma <- 1 / D
beta <- R_0 * sigma

# observation model
rho <- parameter(0.1)
eta <- parameter(10)
Cases <- data()
Cases ~ NegativeBinomial(size = eta, mu = rho*incidence)
})
```

### Running the model

Once the model is compiled, it is possible to generate one (or more!) instance of your model by using the `dust_system_create()` and your model generator.

```{r}
#mod <- seir$new(h_times = hol_t, h_values = hol_v)
mod <- dust_system_create(seir,
                          pars = list(h_times = hol_t, h_values = hol_v))
```

```{r}
# A vector with parameter values
# First parameter -> proportion of population initially infected
# Second argument -> R_0
# Third argument -> Proportion of infection detected 
par0 <- c(5e-5, 1.3, 0.1, 10)

# This vector should be passed as a list
dust_system_update_pars(sys = mod,
                        pars = list(alpha=par0[1],
                                    R_0=par0[2],
                                    h_times = hol_t,
                                    h_values= hol_v))
```

Then the model can be run by specifying the time points between which to integrate the ODEs. Note that the first point (here 0) set the inital time of the integration. Once the time vector t is defined we can run the model by using the "$run(t)" method.
```{r}
t <- c(0,data$Days)
dust_system_set_state_initial(mod)
y <- dust_system_simulate(mod, t)
```

Let's plot the I compartment over time.

```{r}
plot(t, dust_unpack_state(mod, y)$incidence, type = "l", col = "red", lwd = 2)
```

It looks a bit like the actual epidemic; a good sign, but let compare our model with the data.

```{r}
plot(t, dust_unpack_state(mod, y)$incidence, type = "l", col = "red", lwd = 2)
points(data$Days, data$Cases)
```

## The MCMC pipeline

We now have a dataset and a working model, all loaded in our R environment. We need to link the output of this model with the data in order to derive a likelihood function. As we are working within a Bayesian framework we also need to define the prior distribution for our parameters. 

### Setting up the likelihood

The likelihood is a function that takes a parameter as argument and return the probability density that our model (transmission model + observation) generates exactly the observed data. This number is usually very small as it is the product of each individual observation density given the model. As opposed to the example in the lecture, we thus always work using the log-density values rather than the density. Products then become sums and divisions become differences.

```{r}
data$time <- data$Days
filter <- dust_unfilter_create(seir, data = data, time_start = 0)
dust_likelihood_run(filter, list(alpha=par0[1],
                                    R_0=par0[2],
                                    h_times = hol_t,
                                    h_values= hol_v))
```

This is the likelihood associated with the dust model written using odin. We know need to move this into the monty statistical world to be able to integrate this into our pipeline. For this use the `packer` concept that allows us to definne a one-to-one translation between the two worlds.

```{r}
packer <- monty_packer(c("alpha", "R_0", "rho", "eta"),
                       fixed = list(h_times = hol_t,h_values= hol_v))
```

This defines two functions that allows to 'pack' (define) and 'unpack' (define) our parameters.

Now building the `monty` model for the likelihood of the model is very easy:

```{r}
likelihood <- dust_likelihood_monty(filter, packer)
```

### Setting up the prior distribution

We similarly set up a log_prior function. Note that we use the function to exclude impossible values such as proportion below 0 or above 1. This is to avoid to get logdensity of -infinity given that these values would have a density value of 0 (=impossible).

The other ingredient we need is a prior. This we can construct with monty_dsl as before, note that the names are matching the parameters from the likelihood as it should be.

```{r}
prior <- monty_dsl({
  alpha ~ Beta(a = 4e-4, b = 2)
  R_0 ~ Gamma(2, 0.7)
  rho ~ Uniform(0, 1)
  eta ~ Exponential(mean = 1000)
})
```

Let's inspect the our prior `monty` model
```{r}
prior
```

As it has been built using the monty DSL, it comes with gradient calculation, direct sampling (so no need to use one of the Monte Carlo algorithm) and accepts multiple parameters.

Let's try to generate samples and check that they match our prior distribution:
```{r}
n_streams <- 10000
r <- monty_rng$new(n_streams = n_streams)
prior_samples <- matrix(monty_model_direct_sample(prior, r), nrow = n_streams)
colnames(prior_samples) <- prior$parameters
```

```{r}
bayesplot::mcmc_pairs(prior_samples)
```

### Setting up the posterior

Our posterior is the product of the likelihood and prior, or the sum of their logs:

```{r}
posterior <- likelihood + prior
```

### Chosing the sampler

Next, we define a sampler; we'll start with a random walk with a fairly arbitrary diagonal proposal matrix:

```{r}
vcv <- diag(c(5e-10,5e-5,1e-5,1))
sampler <- monty_sampler_adaptive(vcv, initial_vcv_weight = 100)
```

We start this off, using explicit initial conditions

```{r}
#| cache: true
init_values <- packer$pack(list(alpha=par0[1],
                  R_0=par0[2],
                  rho=0.1,
                  eta=10,
                  h_times = hol_t,
                  h_values= hol_v))
init_values <- c(3.05e-5,1.354,0.04, 10)
samples <- monty_sample(posterior, sampler, 10000, initial = init_values,
                        n_chains = 3)
```

::: {.callout-note}
We have used explicit initial conditions here, which might not be what you want in all situations.  Better might be to sample from the prior, but we have not yet implemented support to try a few points from the sample before getting a point with finite density, which is really needed here.
:::

Here the log posterior density of our three chains over time, showing a rapid improvement in the posterior probability density followed by what might be reasonable (but not great) mixing:

```{r}
matplot(samples$density, type = "l", lty = 1,
        xlab = "Sample", ylab = "Log posterior probability density")
length(unique(samples$density))/length(samples$density)
```
```{r}
samples_df <- posterior::as_draws_df(samples)
posterior::summarise_draws(samples_df)
```

```{r}
bayesplot::mcmc_pairs(samples_df)
```

Finally we can get an idea of the fit of the samples by looking at 100 trajectories from the samples.

```{r}
n_groups <- 1000
i_rand <- sample(dim(samples$pars)[2],n_groups)
list_par <- lapply(i_rand, FUN = function(i) packer$unpack(samples$pars[,i,sample(3,1)]))

mod_mult <- dust_system_create(seir,
                          pars = list_par, n_groups = n_groups)

t <- c(0,data$Days)
dust_system_set_state_initial(mod_mult)
y <- dust_system_simulate(mod_mult, t)
y <- dust_unpack_state(mod_mult, y)

matplot(t, t(y$incidence), type = "l", lty = 1, col = "#00008822",
        xlab = "Time", ylab = "Infected population")
points(data$Days, data$Cases/mean(samples$pars["rho",,]), pch=19, col="red")
```

